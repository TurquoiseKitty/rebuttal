{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c85594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import tqdm\n",
    "import six\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from helper import set_seeds\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.datasets import get_scaled_data, get_synthetic_data\n",
    "from utils.q_model_ens import QModelEns, MSEModel\n",
    "from losses import batch_qr_loss, batch_interval_loss\n",
    "import helper\n",
    "from helper import SYN_DATA, REAL_DATA\n",
    "\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"MKL_CBWR\"] = 'AUTO'\n",
    "\n",
    "results_path = helper.results_path\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "def get_loss_fn(loss_name):\n",
    "    if loss_name == 'batch_qr' or loss_name == 'batch_wqr':\n",
    "        fn = batch_qr_loss\n",
    "    elif loss_name == 'batch_int':\n",
    "        fn = batch_interval_loss\n",
    "    else:\n",
    "        raise ValueError('loss arg not valid')\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=None,\n",
    "                        help='random seed')\n",
    "\n",
    "    parser.add_argument('--seed_begin', type=int, default=None,\n",
    "                        help='random seed')\n",
    "    parser.add_argument('--seed_end', type=int, default=None,\n",
    "                        help='random seed')\n",
    "\n",
    "    parser.add_argument('--data', type=str, default='',\n",
    "                        help='dataset to use')\n",
    "\n",
    "    parser.add_argument('--num_q', type=int, default=30,\n",
    "                        help='number of quantiles you want to sample each step')\n",
    "    parser.add_argument('--gpu', type=int, default=1,\n",
    "                        help='gpu num to use')\n",
    "\n",
    "    parser.add_argument('--num_ep', type=int, default=10000,\n",
    "                        help='number of epochs')\n",
    "    parser.add_argument('--nl', type=int, default=2,\n",
    "                        help='number of layers')\n",
    "    parser.add_argument('--hs', type=int, default=64,\n",
    "                        help='hidden size')\n",
    "\n",
    "    parser.add_argument('--dropout', type=float, default=0,\n",
    "                        help='dropout ratio of the dropout level')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.0,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--bs', type=int, default=1024,\n",
    "                        help='batch size')\n",
    "    parser.add_argument('--wait', type=int, default=200,\n",
    "                        help='how long to wait for lower validation loss')\n",
    "\n",
    "    parser.add_argument('--loss', type=str,\n",
    "                        help='specify type of loss')\n",
    "\n",
    "    parser.add_argument('--corr_mult', type=float, default=0.,\n",
    "                        help='correlation penalty multiplier')\n",
    "\n",
    "    parser.add_argument('--hsic_mult', type=float, default=0.,\n",
    "                        help='correlation penalty multiplier')\n",
    "\n",
    "    parser.add_argument('--ds_type', type=str, default=\"\",\n",
    "                        help='type of data set. real or synthetic. REAL for real. SYN for synthetic')\n",
    "\n",
    "    parser.add_argument('--test_ratio', type=float, default=0.4,\n",
    "                        help='ratio of test set size')\n",
    "\n",
    "    parser.add_argument('--save_training_results', type=int, default=0,\n",
    "                        help='1 for saving results during training, or 0 for not saving')\n",
    "\n",
    "    parser.add_argument('--method', type=str, default='QR',\n",
    "                        help='method to use (QR or qr_forest)')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n",
    "    device_name = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device_name)\n",
    "    args.device = device\n",
    "    if args.method not in ['QR', 'qr_forest']:\n",
    "        raise ValueError('method arg not valid')\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def update_results_during_training(y_upper, y_lower, x, y, set_name, results_dict, alpha):\n",
    "    with torch.no_grad():\n",
    "        if len(x) == 0 or len(y) == 0:\n",
    "            return\n",
    "        y = y.reshape(-1).to(device)\n",
    "        idx = np.random.permutation(len(x))  # [:len(xx)]\n",
    "        x = x[idx].to(device)\n",
    "        quantiles = torch.Tensor([alpha / 2, 1 - alpha / 2]).to(device)\n",
    "        \n",
    "\n",
    "        if torch.is_tensor(y):\n",
    "            curr_y = y.cpu().detach().numpy()[idx]\n",
    "        else:\n",
    "            curr_y = y[idx]\n",
    "        in_the_range = ((curr_y >= y_lower) & (curr_y <= y_upper))\n",
    "        lengths = (y_upper - y_lower)\n",
    "\n",
    "        if 'pearsons_correlation' + '_over_' + set_name not in results_dict:\n",
    "            results_dict['pearsons_correlation' + '_over_' + set_name] = []\n",
    "\n",
    "        results_dict['pearsons_correlation' + '_over_' + set_name] += [\n",
    "            stats.pearsonr(in_the_range, lengths)[0]]\n",
    "\n",
    "        if 'coverage' + '_over_' + set_name not in results_dict:\n",
    "            results_dict['coverage' + '_over_' + set_name] = []\n",
    "\n",
    "        results_dict['coverage' + '_over_' + set_name] += [np.mean(in_the_range)]\n",
    "\n",
    "        if 'interval_lengths' + '_over_' + set_name not in results_dict:\n",
    "            results_dict['interval_lengths' + '_over_' + set_name] = []\n",
    "\n",
    "        results_dict['interval_lengths' + '_over_' + set_name] += [np.mean(lengths)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a87d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading meps_19\n",
      "5\n",
      "{'MACE_Loss': 0.031568244099617004, 'AGCE_Loss': 0.10430365055799484, 'CheckScore': 0.12330325692892075}\n",
      "10\n",
      "{'MACE_Loss': 0.02072712406516075, 'AGCE_Loss': 0.09031034260988235, 'CheckScore': 0.11724480241537094}\n",
      "20\n",
      "{'MACE_Loss': 0.008647356182336807, 'AGCE_Loss': 0.11692307889461517, 'CheckScore': 0.11700228601694107}\n",
      "50\n",
      "{'MACE_Loss': 0.010033221915364265, 'AGCE_Loss': 0.06763588637113571, 'CheckScore': 0.11715546250343323}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "{'MACE_Loss': 0.010636065155267715, 'AGCE_Loss': 0.08348485082387924, 'CheckScore': 0.11720244586467743}\n",
      "150\n",
      "{'MACE_Loss': 0.01083358284085989, 'AGCE_Loss': 0.09828563779592514, 'CheckScore': 0.11719542741775513}\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading meps_20\n",
      "5\n",
      "{'MACE_Loss': 0.03302071616053581, 'AGCE_Loss': 0.10008786618709564, 'CheckScore': 0.15000073611736298}\n",
      "10\n",
      "{'MACE_Loss': 0.022303583100438118, 'AGCE_Loss': 0.07036971300840378, 'CheckScore': 0.14997360110282898}\n",
      "20\n",
      "{'MACE_Loss': 0.009158487431704998, 'AGCE_Loss': 0.09048483520746231, 'CheckScore': 0.14989732205867767}\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:05<00:00,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MACE_Loss': 0.008125373162329197, 'AGCE_Loss': 0.056538376957178116, 'CheckScore': 0.1497844159603119}\n",
      "100\n",
      "{'MACE_Loss': 0.008563229814171791, 'AGCE_Loss': 0.04817577078938484, 'CheckScore': 0.14970393478870392}\n",
      "150\n",
      "{'MACE_Loss': 0.008814041502773762, 'AGCE_Loss': 0.040113117545843124, 'CheckScore': 0.14965838193893433}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading meps_21\n",
      "5\n",
      "{'MACE_Loss': 0.07681116461753845, 'AGCE_Loss': 0.13475805521011353, 'CheckScore': 0.15599632263183594}\n",
      "10\n",
      "{'MACE_Loss': 0.060255035758018494, 'AGCE_Loss': 0.09554856270551682, 'CheckScore': 0.15544170141220093}\n",
      "20\n",
      "{'MACE_Loss': 0.04824228957295418, 'AGCE_Loss': 0.13871794939041138, 'CheckScore': 0.15455974638462067}\n",
      "50\n",
      "{'MACE_Loss': 0.03997792303562164, 'AGCE_Loss': 0.1491723358631134, 'CheckScore': 0.15417414903640747}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "{'MACE_Loss': 0.037829577922821045, 'AGCE_Loss': 0.07804243266582489, 'CheckScore': 0.15415546298027039}\n",
      "150\n",
      "{'MACE_Loss': 0.0375482514500618, 'AGCE_Loss': 0.07106993347406387, 'CheckScore': 0.15417298674583435}\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading facebook_1\n",
      "5\n",
      "{'MACE_Loss': 0.026011407375335693, 'AGCE_Loss': 0.06906558573246002, 'CheckScore': 0.11090192943811417}\n",
      "10\n",
      "{'MACE_Loss': 0.024971654638648033, 'AGCE_Loss': 0.06378703564405441, 'CheckScore': 0.10838855803012848}\n",
      "20\n",
      "{'MACE_Loss': 0.0244070403277874, 'AGCE_Loss': 0.07220427691936493, 'CheckScore': 0.10851825028657913}\n",
      "50\n",
      "{'MACE_Loss': 0.024301471188664436, 'AGCE_Loss': 0.06685703247785568, 'CheckScore': 0.10851526260375977}\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MACE_Loss': 0.024340569972991943, 'AGCE_Loss': 0.05918869748711586, 'CheckScore': 0.10850077122449875}\n",
      "150\n",
      "{'MACE_Loss': 0.024301469326019287, 'AGCE_Loss': 0.07705727219581604, 'CheckScore': 0.10850095748901367}\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading facebook_2\n",
      "5\n",
      "{'MACE_Loss': 0.016712097451090813, 'AGCE_Loss': 0.029018931090831757, 'CheckScore': 0.11480104178190231}\n",
      "10\n",
      "{'MACE_Loss': 0.013738510198891163, 'AGCE_Loss': 0.0515252910554409, 'CheckScore': 0.1148904412984848}\n",
      "20\n",
      "{'MACE_Loss': 0.012179391458630562, 'AGCE_Loss': 0.039712172001600266, 'CheckScore': 0.11473292857408524}\n",
      "50\n",
      "{'MACE_Loss': 0.01035897433757782, 'AGCE_Loss': 0.04481220245361328, 'CheckScore': 0.11474768072366714}\n",
      "100\n",
      "{'MACE_Loss': 0.00984327495098114, 'AGCE_Loss': 0.03219832479953766, 'CheckScore': 0.11474159359931946}\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MACE_Loss': 0.009774412959814072, 'AGCE_Loss': 0.046305425465106964, 'CheckScore': 0.11474265158176422}\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading blog_data\n",
      "5\n",
      "{'MACE_Loss': 0.028894590213894844, 'AGCE_Loss': 0.051696863025426865, 'CheckScore': 0.15224525332450867}\n",
      "10\n",
      "{'MACE_Loss': 0.01265603955835104, 'AGCE_Loss': 0.05269784852862358, 'CheckScore': 0.1384609192609787}\n",
      "20\n",
      "{'MACE_Loss': 0.006692203693091869, 'AGCE_Loss': 0.02705974318087101, 'CheckScore': 0.13754688203334808}\n",
      "50\n",
      "{'MACE_Loss': 0.0065119918435812, 'AGCE_Loss': 0.036810193210840225, 'CheckScore': 0.1374085694551468}\n",
      "100\n",
      "{'MACE_Loss': 0.0074704778380692005, 'AGCE_Loss': 0.04855223372578621, 'CheckScore': 0.13744112849235535}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:35<00:00, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "{'MACE_Loss': 0.007481812033802271, 'AGCE_Loss': 0.046855051070451736, 'CheckScore': 0.13742966949939728}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from Experiments.EXP1.TestPerform import testPerform_projKernel\n",
    "from sklearn import random_projection\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "from src.kernel_methods import kernel_estimator\n",
    "from losses import independence_penalty\n",
    "\n",
    "\n",
    "\n",
    "def mass_exp(DATA_NAMES, SEEDS = range(0, 1), recal = 0.3, ker_range = [10, 20, 30, 50], resample_perce = 5):\n",
    "\n",
    "    data_type = REAL_DATA\n",
    "\n",
    "    # DATA_NAMES = ['meps_19', 'meps_20', 'meps_21', 'facebook_1', 'facebook_2','blog_data']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    save_results_during_training = True\n",
    "\n",
    "\n",
    "    alpha = 0.1\n",
    "\n",
    "\n",
    "    for d in DATA_NAMES:\n",
    "        print(\"-------------------------------------------------\")\n",
    "        if save_results_during_training:\n",
    "            results_during_training = {}\n",
    "            for s in SEEDS:\n",
    "                results_during_training[s] = {}\n",
    "\n",
    "        for s in tqdm.tqdm(SEEDS):\n",
    "\n",
    "            arg_data = str(d)\n",
    "            arg_seed = s\n",
    "\n",
    "            set_seeds(arg_seed)\n",
    "            data_args = Namespace(dataset=arg_data, seed=arg_seed)\n",
    "\n",
    "            # if data_type == REAL_DATA:\n",
    "            # Fetching data\n",
    "            data_out = get_scaled_data(arg_data, arg_seed, recal_prop = recal)\n",
    "            x_train, y_train = data_out.x_train, data_out.y_train\n",
    "            unscaled_x_train = None\n",
    "            unscaled_x_test = None\n",
    "            minority_group_uncertainty = None\n",
    "            group_feature = None\n",
    "\n",
    "\n",
    "            x_tr, x_va, x_te, y_tr, y_va, y_te, y_al = \\\n",
    "                data_out.x_tr, data_out.x_va, data_out.x_te, data_out.y_tr, \\\n",
    "                data_out.y_va, data_out.y_te, data_out.y_al\n",
    "            \n",
    "            if resample_perce > 0:\n",
    "                \n",
    "                x_va = x_va[:int(len(x_va)/resample_perce)]\n",
    "                y_va = y_va[:int(len(y_va)/resample_perce)]\n",
    "                x_te = x_te[:int(len(x_te)/resample_perce)]\n",
    "                y_te = y_te[:int(len(y_te)/resample_perce)]\n",
    "            \n",
    "            \n",
    "            print(\"finish loading \"+d)\n",
    "\n",
    "\n",
    "            ## our method\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "\n",
    "            depth = 10\n",
    "\n",
    "            train_X = x_tr.numpy()\n",
    "            train_Y = y_tr.view(-1).numpy()\n",
    "            test_X = x_te\n",
    "            test_Y = y_te.cuda().view(-1)\n",
    "            recal_X = x_va\n",
    "            recal_Y = y_va.cuda().view(-1)\n",
    "\n",
    "            rf_model = RandomForestRegressor(max_depth=depth, random_state=0)\n",
    "            rf_model.fit(train_X, train_Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            n_component = 20\n",
    "            transformer = random_projection.GaussianRandomProjection(n_components = n_component)\n",
    "            reformer = lambda x : torch.Tensor(transformer.fit_transform(x.cpu().numpy()))\n",
    "\n",
    "            for width in ker_range:\n",
    "                print(width)\n",
    "\n",
    "                record = testPerform_projKernel(\n",
    "                    test_X, test_Y, recal_X, recal_Y, \n",
    "                    model_name = \"RFKernel_RandomProj\", model= rf_model, reformer= reformer, wid = width) \n",
    "\n",
    "                print(record)\n",
    "\n",
    "\n",
    "\n",
    "                recal_mean = torch.Tensor(rf_model.predict(recal_X.cpu().numpy())).cuda()\n",
    "                test_mean = torch.Tensor(rf_model.predict(test_X.cpu().numpy())).cuda()\n",
    "\n",
    "\n",
    "\n",
    "                test_Z =  reformer(test_X)\n",
    "\n",
    "                recal_Z = reformer(recal_X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                eps_diffQuants = kernel_estimator(\n",
    "                    test_Z = test_Z.cuda(),\n",
    "                    recal_Z = recal_Z.cuda(),\n",
    "                    recal_epsilon = torch.Tensor(recal_Y - recal_mean).cuda(),\n",
    "                    quants = np.array([alpha/2, 1-alpha/2]),\n",
    "                    wid= width\n",
    "                )\n",
    "\n",
    "                y_diffQuants = (eps_diffQuants + test_mean.view(1,-1).repeat(len(eps_diffQuants),1)).cpu().numpy()\n",
    "\n",
    "                \n",
    "                y_lower = y_diffQuants[0]\n",
    "                y_upper = y_diffQuants[1]\n",
    "                \n",
    "                \n",
    "                # result_vaca = {}\n",
    "\n",
    "                # update_results_during_training(y_upper, y_lower, x_te, y_te, \"test\", result_vaca, alpha)\n",
    "\n",
    "                # print(result_vaca)\n",
    "                \n",
    "                # print(\"HSIC: \", independence_penalty(y_te.cpu(), torch.Tensor(y_lower).view(1,-1), torch.Tensor(y_upper).view(1,-1)))\n",
    "\n",
    "\n",
    "            # --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "mass_exp(DATA_NAMES = ['meps_19', 'meps_20', 'meps_21', 'facebook_1', 'facebook_2','blog_data'], SEEDS = range(0, 1), recal = 0.3, ker_range = [5, 10, 20, 50, 100, 150], resample_perce = 4)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a4cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
