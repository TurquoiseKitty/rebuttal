{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1eae0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import tqdm\n",
    "import six\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from helper import set_seeds\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.datasets import get_scaled_data, get_synthetic_data\n",
    "from utils.q_model_ens import QModelEns, MSEModel\n",
    "from losses import batch_qr_loss, batch_interval_loss\n",
    "import helper\n",
    "from helper import SYN_DATA, REAL_DATA\n",
    "\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"MKL_CBWR\"] = 'AUTO'\n",
    "\n",
    "results_path = helper.results_path\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "def get_loss_fn(loss_name):\n",
    "    if loss_name == 'batch_qr' or loss_name == 'batch_wqr':\n",
    "        fn = batch_qr_loss\n",
    "    elif loss_name == 'batch_int':\n",
    "        fn = batch_interval_loss\n",
    "    else:\n",
    "        raise ValueError('loss arg not valid')\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=None,\n",
    "                        help='random seed')\n",
    "\n",
    "    parser.add_argument('--seed_begin', type=int, default=None,\n",
    "                        help='random seed')\n",
    "    parser.add_argument('--seed_end', type=int, default=None,\n",
    "                        help='random seed')\n",
    "\n",
    "    parser.add_argument('--data', type=str, default='',\n",
    "                        help='dataset to use')\n",
    "\n",
    "    parser.add_argument('--num_q', type=int, default=30,\n",
    "                        help='number of quantiles you want to sample each step')\n",
    "    parser.add_argument('--gpu', type=int, default=1,\n",
    "                        help='gpu num to use')\n",
    "\n",
    "    parser.add_argument('--num_ep', type=int, default=10000,\n",
    "                        help='number of epochs')\n",
    "    parser.add_argument('--nl', type=int, default=2,\n",
    "                        help='number of layers')\n",
    "    parser.add_argument('--hs', type=int, default=64,\n",
    "                        help='hidden size')\n",
    "\n",
    "    parser.add_argument('--dropout', type=float, default=0,\n",
    "                        help='dropout ratio of the dropout level')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.0,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--bs', type=int, default=1024,\n",
    "                        help='batch size')\n",
    "    parser.add_argument('--wait', type=int, default=200,\n",
    "                        help='how long to wait for lower validation loss')\n",
    "\n",
    "    parser.add_argument('--loss', type=str,\n",
    "                        help='specify type of loss')\n",
    "\n",
    "    parser.add_argument('--corr_mult', type=float, default=0.,\n",
    "                        help='correlation penalty multiplier')\n",
    "\n",
    "    parser.add_argument('--hsic_mult', type=float, default=0.,\n",
    "                        help='correlation penalty multiplier')\n",
    "\n",
    "    parser.add_argument('--ds_type', type=str, default=\"\",\n",
    "                        help='type of data set. real or synthetic. REAL for real. SYN for synthetic')\n",
    "\n",
    "    parser.add_argument('--test_ratio', type=float, default=0.4,\n",
    "                        help='ratio of test set size')\n",
    "\n",
    "    parser.add_argument('--save_training_results', type=int, default=0,\n",
    "                        help='1 for saving results during training, or 0 for not saving')\n",
    "\n",
    "    parser.add_argument('--method', type=str, default='QR',\n",
    "                        help='method to use (QR or qr_forest)')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n",
    "    device_name = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device_name)\n",
    "    args.device = device\n",
    "    if args.method not in ['QR', 'qr_forest']:\n",
    "        raise ValueError('method arg not valid')\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def update_results_during_training(y_upper, y_lower, x, y, set_name, results_dict, alpha):\n",
    "    with torch.no_grad():\n",
    "        if len(x) == 0 or len(y) == 0:\n",
    "            return\n",
    "        y = y.reshape(-1).to(device)\n",
    "        idx = np.random.permutation(len(x))  # [:len(xx)]\n",
    "        x = x[idx].to(device)\n",
    "        quantiles = torch.Tensor([alpha / 2, 1 - alpha / 2]).to(device)\n",
    "        \n",
    "\n",
    "        if torch.is_tensor(y):\n",
    "            curr_y = y.cpu().detach().numpy()[idx]\n",
    "        else:\n",
    "            curr_y = y[idx]\n",
    "        in_the_range = ((curr_y >= y_lower) & (curr_y <= y_upper))\n",
    "        lengths = (y_upper - y_lower)\n",
    "\n",
    "        if 'pearsons_correlation' + '_over_' + set_name not in results_dict:\n",
    "            results_dict['pearsons_correlation' + '_over_' + set_name] = []\n",
    "\n",
    "        results_dict['pearsons_correlation' + '_over_' + set_name] += [\n",
    "            stats.pearsonr(in_the_range, lengths)[0]]\n",
    "\n",
    "        if 'coverage' + '_over_' + set_name not in results_dict:\n",
    "            results_dict['coverage' + '_over_' + set_name] = []\n",
    "\n",
    "        results_dict['coverage' + '_over_' + set_name] += [np.mean(in_the_range)]\n",
    "\n",
    "        if 'interval_lengths' + '_over_' + set_name not in results_dict:\n",
    "            results_dict['interval_lengths' + '_over_' + set_name] = []\n",
    "\n",
    "        results_dict['interval_lengths' + '_over_' + set_name] += [np.mean(lengths)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On dataset meps_19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:18<00:00, 78.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training ens at EP 563\n",
      "{'MACE_Loss': 0.030589619651436806, 'AGCE_Loss': 0.05355070158839226, 'CheckScore': 0.11384144425392151}\n",
      "--------------------------------------------------\n",
      "On dataset meps_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:51<00:00, 111.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training ens at EP 743\n",
      "{'MACE_Loss': 0.03574376180768013, 'AGCE_Loss': 0.05235308036208153, 'CheckScore': 0.12034844607114792}\n",
      "--------------------------------------------------\n",
      "On dataset meps_21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:17<00:00, 77.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training ens at EP 564\n",
      "{'MACE_Loss': 0.04197763651609421, 'AGCE_Loss': 0.06319630146026611, 'CheckScore': 0.10958028584718704}\n",
      "--------------------------------------------------\n",
      "On dataset facebook_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP:0\n"
     ]
    }
   ],
   "source": [
    "from utils.penalty_multipliers import real_corr_per_dataset_per_loss\n",
    "from Experiments.EXP1.trainer import model_callByName, loss_callByName\n",
    "\n",
    "POSSIBLE_REAL_DATA_NAMES = ['kin8nm', 'naval', 'meps_19', 'meps_20', 'meps_21', 'facebook_1', 'facebook_2',\n",
    "                            'blog_data', 'bio', 'scaled_bio', 'bike']\n",
    "\n",
    "\n",
    "data_type = REAL_DATA\n",
    "\n",
    "DATA_NAMES = ['meps_19', 'meps_20', 'meps_21', 'facebook_1', 'facebook_2', 'blog_data']\n",
    "\n",
    "\n",
    "\n",
    "SEEDS = range(0, 1)\n",
    "\n",
    "save_results_during_training = True\n",
    "\n",
    "\n",
    "arg_loss = 'batch_int'\n",
    "TRAINING_OVER_ALL_QUANTILES = True\n",
    "\n",
    "    \n",
    "\n",
    "for d in DATA_NAMES:\n",
    "    \n",
    "    print(\"On dataset \"+ d)\n",
    "\n",
    "    for s in tqdm.tqdm(SEEDS):\n",
    "\n",
    "        arg_data = d\n",
    "        arg_seed = s\n",
    "        \n",
    "        \n",
    "        set_seeds(arg_seed)\n",
    "        data_args = Namespace(dataset=arg_data, seed=arg_seed)\n",
    "\n",
    "        # if data_type == REAL_DATA:\n",
    "        # Fetching data\n",
    "        data_out = get_scaled_data(arg_data, arg_seed, recal_prop = 0.1)\n",
    "        x_train, y_train = data_out.x_train, data_out.y_train\n",
    "        unscaled_x_train = None\n",
    "        unscaled_x_test = None\n",
    "        minority_group_uncertainty = None\n",
    "        group_feature = None\n",
    "\n",
    "\n",
    "        x_tr, x_va, x_te, y_tr, y_va, y_te, y_al = \\\n",
    "            data_out.x_tr, data_out.x_va, data_out.x_te, data_out.y_tr, \\\n",
    "            data_out.y_va, data_out.y_te, data_out.y_al\n",
    "        \n",
    "        x_va, y_va = x_va.to(device), y_va.to(device)\n",
    "        x_tr, y_tr = x_tr.to(device), y_tr.to(device)\n",
    "        y_te, x_te = y_te.to(device), x_te.to(device)\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        x_test = x_te\n",
    "        y_test = y_te\n",
    "\n",
    "        w_tr, w_va, get_tr_weights = helper.get_wqr_weights(arg_loss, x_tr, y_tr, x_va, y_va, device = device)\n",
    "        \n",
    "        y_range = (y_al.max() - y_al.min()).item()\n",
    "\n",
    "        # creating the model\n",
    "        num_tr = x_tr.shape[0]\n",
    "        dim_x = x_tr.shape[1]\n",
    "        dim_y = y_tr.shape[1]\n",
    "\n",
    "        model_ens = QModelEns(input_size=dim_x + 1, output_size=dim_y,\n",
    "                              hidden_size=64, num_layers=2, dropout=0,\n",
    "                              lr=1e-3, wd=0,\n",
    "                              num_ens=1, device=device)\n",
    "\n",
    "        loader = DataLoader(helper.IndexedDataset(x_tr, y_tr),\n",
    "                            shuffle=True,\n",
    "                            batch_size=1024)\n",
    "\n",
    "        # Loss function\n",
    "        loss_fn = get_loss_fn(arg_loss)\n",
    "        batch_loss = True if 'batch' in arg_loss else False\n",
    "        \"\"\" train loop \"\"\"\n",
    "        tr_loss_list = []\n",
    "        va_loss_list = []\n",
    "        te_loss_list = []\n",
    "        batch_size = 1024\n",
    "        for ep in range(10000):\n",
    "\n",
    "            if model_ens.done_training:\n",
    "                print('Done training ens at EP {}'.format(ep))\n",
    "                break\n",
    "\n",
    "            # Take train step\n",
    "            ep_train_loss = []  # list of losses from each batch, for one epoch\n",
    "            epoch_loss = []\n",
    "\n",
    "            for xi, yi, index in loader:\n",
    "                if TRAINING_OVER_ALL_QUANTILES:\n",
    "                    q_list = torch.rand(30)\n",
    "                else:\n",
    "                    q_list = torch.Tensor([alpha / 2])\n",
    "                    \n",
    "                arg_corr_mult = real_corr_per_dataset_per_loss[\"int\"][d]\n",
    "                loss = model_ens.loss(loss_fn, xi, yi, q_list,\n",
    "                                      batch_q=batch_loss,\n",
    "                                      take_step=True, args=arg_corr_mult, weights=get_tr_weights(index))\n",
    "                ep_train_loss.append(loss)\n",
    "\n",
    "            ep_tr_loss = np.nanmean(np.stack(ep_train_loss, axis=0), axis=0)\n",
    "            tr_loss_list.append(ep_tr_loss)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Validation loss\n",
    "            # x_va, y_va = x_va.to(args.device), y_va.to(args.device)\n",
    "            if TRAINING_OVER_ALL_QUANTILES:\n",
    "                va_te_q_list = torch.linspace(0.01, 0.99, 99)\n",
    "            else:\n",
    "                va_te_q_list = torch.Tensor([alpha / 2, 1 - alpha / 2])\n",
    "\n",
    "            ep_va_loss = model_ens.update_va_loss(\n",
    "                loss_fn, x_va, y_va, va_te_q_list,\n",
    "                batch_q=batch_loss, curr_ep=ep, num_wait=200,\n",
    "                args=arg_corr_mult, weights=w_va\n",
    "            )\n",
    "            va_loss_list.append(ep_va_loss)\n",
    "\n",
    "            # Printing some losses\n",
    "            if (ep % 1000 == 0):\n",
    "                print('EP:{}'.format(ep))\n",
    "                pass\n",
    "        # Move everything to cpu\n",
    "        x_tr, y_tr, x_va, y_va, x_te, y_te = \\\n",
    "            x_tr.cpu(), y_tr.cpu(), x_va.cpu(), y_va.cpu(), x_te.cpu(), y_te.cpu()\n",
    "        model_ens.use_device(torch.device('cpu'))\n",
    "\n",
    "        quantiles = torch.Tensor(np.linspace(0.01,0.99,99))\n",
    "        test_preds = model_ens.predict_q(\n",
    "            x_te, quantiles, ens_pred_type='conf',\n",
    "            recal_model=None, recal_type=None\n",
    "        )\n",
    "        test_preds.detach().cpu().numpy()\n",
    "\n",
    "        # print(test_preds.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        test_Y = y_te.view(-1)\n",
    "        ret = {}\n",
    "        val_criterias = [\n",
    "                \"MACE_Loss\", \"AGCE_Loss\", \"CheckScore\"\n",
    "            ]  \n",
    "\n",
    "        for key in val_criterias:\n",
    "\n",
    "            real_loss = loss_callByName[key]\n",
    "\n",
    "            real_err = real_loss(torch.permute(test_preds, (1,0)), test_Y, q_list = np.linspace(0.01,0.99,99)).item()\n",
    "\n",
    "            if isinstance(real_err, torch.Tensor):\n",
    "\n",
    "                real_err = real_err.item()\n",
    "\n",
    "            ret[key] = real_err\n",
    "\n",
    "\n",
    "        print(ret)\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991ea2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
